{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO on Humanoid-v4 using only Numpy\n",
    "\n",
    "This notebook implements the Proximal Policy Optimization (PPO) algorithm from scratch using only `numpy` for the neural networks and optimization. The environment used is `Humanoid-v4` from Gymnasium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2. / input_dim)\n",
    "        self.bias = np.zeros(output_dim)\n",
    "        # Adam parameters\n",
    "        self.m_w, self.v_w = np.zeros_like(self.weights), np.zeros_like(self.weights)\n",
    "        self.m_b, self.v_b = np.zeros_like(self.bias), np.zeros_like(self.bias)\n",
    "        self.t = 0\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, grad_output, lr):\n",
    "        self.t += 1\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0)\n",
    "        \n",
    "        # Adam Update\n",
    "        beta1, beta2, eps = 0.9, 0.999, 1e-8\n",
    "        self.m_w = beta1 * self.m_w + (1 - beta1) * grad_weights\n",
    "        self.v_w = beta2 * self.v_w + (1 - beta2) * grad_weights**2\n",
    "        m_hat_w = self.m_w / (1 - beta1**self.t)\n",
    "        v_hat_w = self.v_w / (1 - beta2**self.t)\n",
    "        self.weights -= lr * m_hat_w / (np.sqrt(v_hat_w) + eps)\n",
    "\n",
    "        self.m_b = beta1 * self.m_b + (1 - beta1) * grad_bias\n",
    "        self.v_b = beta2 * self.v_b + (1 - beta2) * grad_bias**2\n",
    "        m_hat_b = self.m_b / (1 - beta1**self.t)\n",
    "        v_hat_b = self.v_b / (1 - beta2**self.t)\n",
    "        self.bias -= lr * m_hat_b / (np.sqrt(v_hat_b) + eps)\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_relu(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def log_prob(x, mean, std):\n",
    "    var = std**2\n",
    "    return -0.5 * np.sum((x - mean)**2 / (var + 1e-8), axis=-1) - 0.5 * x.shape[-1] * np.log(2 * np.pi) - np.sum(np.log(std + 1e-8), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, obs_dim, act_dim, lr=1e-3, gamma=0.99, clip_ratio=0.2, lam=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.lam = lam\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Policy Network (2 layers of 64)\n",
    "        self.p_l1 = Dense(obs_dim, 64)\n",
    "        self.p_l2 = Dense(64, 64)\n",
    "        self.p_out = Dense(64, act_dim)\n",
    "        self.log_std = np.zeros(act_dim) # Trainable log_std\n",
    "        \n",
    "        # Value Network (2 layers of 64)\n",
    "        self.v_l1 = Dense(obs_dim, 64)\n",
    "        self.v_l2 = Dense(64, 64)\n",
    "        self.v_out = Dense(64, 1)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        if obs.ndim == 1: obs = obs[np.newaxis, :]\n",
    "        h1 = relu(self.p_l1.forward(obs))\n",
    "        h2 = relu(self.p_l2.forward(h1))\n",
    "        mean = self.p_out.forward(h2)\n",
    "        std = np.exp(self.log_std)\n",
    "        action = mean + np.random.normal(0, 1, size=mean.shape) * std\n",
    "        return action[0], mean[0], std\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        if obs.ndim == 1: obs = obs[np.newaxis, :]\n",
    "        h1 = relu(self.v_l1.forward(obs))\n",
    "        h2 = relu(self.v_l2.forward(h1))\n",
    "        val = self.v_out.forward(h2)\n",
    "        return val[0, 0]\n",
    "\n",
    "    def update(self, obs, act, ret, adv, old_log_probs):\n",
    "        # Convert to batches if needed, here we assume full batch for simplicity or mini-batches\n",
    "        # For pure numpy simplicity, we do full batch update\n",
    "        \n",
    "        # --- Policy Update ---\n",
    "        # Forward pass\n",
    "        h1 = relu(self.p_l1.forward(obs))\n",
    "        h2 = relu(self.p_l2.forward(h1))\n",
    "        mean = self.p_out.forward(h2)\n",
    "        std = np.exp(self.log_std)\n",
    "        \n",
    "        # New Log Probs\n",
    "        new_log_probs = log_prob(act, mean, std)\n",
    "        ratio = np.exp(new_log_probs - old_log_probs)\n",
    "        \n",
    "        # PPO Loss Gradients\n",
    "        # Loss = -min(ratio * adv, clip(ratio) * adv)\n",
    "        # We need gradient of Loss w.r.t mean and log_std\n",
    "        \n",
    "        surr1 = ratio * adv\n",
    "        surr2 = np.clip(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * adv\n",
    "        \n",
    "        # Gradient of min(surr1, surr2) w.r.t ratio\n",
    "        mask = (surr1 < surr2).astype(float)\n",
    "        d_loss_d_ratio = - (mask * adv + (1 - mask) * 0) # Simplified: if clipped, grad is 0 w.r.t ratio (approx)\n",
    "        # Actually, if clipped, the gradient is 0. If not clipped, it's adv.\n",
    "        # Correct logic: \n",
    "        # if ratio * adv < clipped * adv: grad is adv\n",
    "        # else: grad is 0\n",
    "        \n",
    "        d_ratio = np.where(surr1 < surr2, adv, 0)\n",
    "        \n",
    "        d_log_prob = d_ratio * ratio\n",
    "        \n",
    "        # Gradient of log_prob w.r.t mean: (x - mean) / var\n",
    "        d_mean = d_log_prob[:, np.newaxis] * (act - mean) / (std**2 + 1e-8)\n",
    "        # Gradient of log_prob w.r.t log_std: -1 + ((x - mean)^2 / var)\n",
    "        d_log_std = d_log_prob[:, np.newaxis] * (-1 + (act - mean)**2 / (std**2 + 1e-8))\n",
    "        \n",
    "        # Backprop through Policy MLP\n",
    "        d_out = d_mean\n",
    "        d_h2 = self.p_out.backward(d_out, self.lr)\n",
    "        d_h2_relu = d_h2 * d_relu(h2)\n",
    "        d_h1 = self.p_l2.backward(d_h2_relu, self.lr)\n",
    "        d_h1_relu = d_h1 * d_relu(h1)\n",
    "        self.p_l1.backward(d_h1_relu, self.lr)\n",
    "        \n",
    "        # Update log_std manually (it's a parameter)\n",
    "        # Adam for log_std is skipped for brevity, just SGD\n",
    "        self.log_std += self.lr * np.mean(d_log_std, axis=0)\n",
    "\n",
    "        # --- Value Update ---\n",
    "        # Forward\n",
    "        v_h1 = relu(self.v_l1.forward(obs))\n",
    "        v_h2 = relu(self.v_l2.forward(v_h1))\n",
    "        values = self.v_out.forward(v_h2)\n",
    "        \n",
    "        # MSE Loss: (values - ret)^2\n",
    "        # d_loss_d_val = 2 * (values - ret)\n",
    "        d_val = 2 * (values - ret[:, np.newaxis])\n",
    "        \n",
    "        # Backprop Value\n",
    "        d_v_h2 = self.v_out.backward(d_val, self.lr)\n",
    "        d_v_h2_relu = d_v_h2 * d_relu(v_h2)\n",
    "        d_v_h1 = self.v_l2.backward(d_v_h2_relu, self.lr)\n",
    "        d_v_h1_relu = d_v_h1 * d_relu(v_h1)\n",
    "        self.v_l1.backward(d_v_h1_relu, self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    advs = []\n",
    "    gae = 0\n",
    "    values = np.append(values, 0) # Bootstrap\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i+1] * (1 - dones[i]) - values[i]\n",
    "        gae = delta + gamma * lam * (1 - dones[i]) * gae\n",
    "        advs.insert(0, gae)\n",
    "    return np.array(advs)\n",
    "\n",
    "# Main Loop\n",
    "env = gym.make('Humanoid-v4')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = PPOAgent(obs_dim, act_dim)\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 2048\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    obs, _ = env.reset()\n",
    "    batch_obs, batch_act, batch_rew, batch_val, batch_log_prob, batch_done = [], [], [], [], [], []\n",
    "    \n",
    "    for t in range(steps_per_epoch):\n",
    "        action, mean, std = agent.get_action(obs)\n",
    "        val = agent.get_value(obs)\n",
    "        \n",
    "        next_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        batch_obs.append(obs)\n",
    "        batch_act.append(action)\n",
    "        batch_rew.append(rew)\n",
    "        batch_val.append(val)\n",
    "        batch_log_prob.append(log_prob(action, mean, std))\n",
    "        batch_done.append(done)\n",
    "        \n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "            \n",
    "    # Compute GAE\n",
    "    batch_obs = np.array(batch_obs)\n",
    "    batch_act = np.array(batch_act)\n",
    "    batch_rew = np.array(batch_rew)\n",
    "    batch_val = np.array(batch_val)\n",
    "    batch_done = np.array(batch_done)\n",
    "    \n",
    "    advs = compute_gae(batch_rew, batch_val, batch_done)\n",
    "    returns = advs + batch_val\n",
    "    \n",
    "    # Normalize Advantages\n",
    "    advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "    \n",
    "    # Update\n",
    "    old_log_probs = np.array(batch_log_prob)\n",
    "    agent.update(batch_obs, batch_act, returns, advs, old_log_probs)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Mean Reward: {np.sum(batch_rew)}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
