{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PPO on Humanoid-v4 using PyTorch\n",
                "\n",
                "This notebook implements the Proximal Policy Optimization (PPO) algorithm using **PyTorch**. The environment used is `Humanoid-v4` from Gymnasium. This implementation leverages the GPU if available."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install gymnasium[mujoco] "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gymnasium as gym\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "from torch.distributions import Normal\n",
                "\n",
                "# Set device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
                "    torch.nn.init.orthogonal_(layer.weight, std)\n",
                "    torch.nn.init.constant_(layer.bias, bias_const)\n",
                "    return layer\n",
                "\n",
                "class Agent(nn.Module):\n",
                "    def __init__(self, envs):\n",
                "        super().__init__()\n",
                "        self.critic = nn.Sequential(\n",
                "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
                "            nn.Tanh(),\n",
                "            layer_init(nn.Linear(64, 64)),\n",
                "            nn.Tanh(),\n",
                "            layer_init(nn.Linear(64, 1), std=1.0),\n",
                "        )\n",
                "        self.actor_mean = nn.Sequential(\n",
                "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
                "            nn.Tanh(),\n",
                "            layer_init(nn.Linear(64, 64)),\n",
                "            nn.Tanh(),\n",
                "            layer_init(nn.Linear(64, np.array(envs.single_action_space.shape).prod()), std=0.01),\n",
                "        )\n",
                "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.array(envs.single_action_space.shape).prod()))\n",
                "\n",
                "    def get_value(self, x):\n",
                "        return self.critic(x)\n",
                "\n",
                "    def get_action_and_value(self, x, action=None):\n",
                "        action_mean = self.actor_mean(x)\n",
                "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
                "        action_std = torch.exp(action_logstd)\n",
                "        probs = Normal(action_mean, action_std)\n",
                "        if action is None:\n",
                "            action = probs.sample()\n",
                "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "learning_rate = 3e-4\n",
                "num_steps = 2048\n",
                "batch_size = 2048 # Full batch for simplicity in this demo, usually num_envs * num_steps\n",
                "minibatch_size = 64\n",
                "gamma = 0.99\n",
                "gae_lambda = 0.95\n",
                "clip_coef = 0.2\n",
                "ent_coef = 0.0\n",
                "vf_coef = 0.5\n",
                "max_grad_norm = 0.5\n",
                "total_timesteps = 100000\n",
                "num_epochs = 10\n",
                "\n",
                "# Environment Setup\n",
                "env = gym.make(\"Humanoid-v4\")\n",
                "env = gym.wrappers.ClipAction(env)\n",
                "env = gym.wrappers.NormalizeObservation(env)\n",
                "env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
                "env = gym.wrappers.NormalizeReward(env)\n",
                "env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
                "\n",
                "# Wrap in SyncVectorEnv for compatibility with standard PPO implementations usually expecting vectorized envs\n",
                "# Here we just use a single env but treat it as a batch of 1 for the network\n",
                "env = gym.vector.SyncVectorEnv([lambda: env])\n",
                "\n",
                "agent = Agent(env).to(device)\n",
                "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)\n",
                "\n",
                "# Storage\n",
                "obs = torch.zeros((num_steps, 1) + env.single_observation_space.shape).to(device)\n",
                "actions = torch.zeros((num_steps, 1) + env.single_action_space.shape).to(device)\n",
                "logprobs = torch.zeros((num_steps, 1)).to(device)\n",
                "rewards = torch.zeros((num_steps, 1)).to(device)\n",
                "dones = torch.zeros((num_steps, 1)).to(device)\n",
                "values = torch.zeros((num_steps, 1)).to(device)\n",
                "\n",
                "global_step = 0\n",
                "next_obs = torch.Tensor(env.reset()[0]).to(device)\n",
                "next_done = torch.zeros(1).to(device)\n",
                "\n",
                "num_updates = total_timesteps // num_steps\n",
                "\n",
                "for update in range(1, num_updates + 1):\n",
                "    # Annealing the rate if instructed\n",
                "    frac = 1.0 - (update - 1.0) / num_updates\n",
                "    lrnow = frac * learning_rate\n",
                "    optimizer.param_groups[0][\"lr\"] = lrnow\n",
                "\n",
                "    for step in range(0, num_steps):\n",
                "        global_step += 1\n",
                "        obs[step] = next_obs\n",
                "        dones[step] = next_done\n",
                "\n",
                "        with torch.no_grad():\n",
                "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
                "            values[step] = value.flatten()\n",
                "        actions[step] = action\n",
                "        logprobs[step] = logprob\n",
                "\n",
                "        next_obs, reward, terminations, truncations, infos = env.step(action.cpu().numpy())\n",
                "        next_done = np.logical_or(terminations, truncations)\n",
                "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
                "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
                "\n",
                "    # Bootstrap value if not done\n",
                "    with torch.no_grad():\n",
                "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
                "        advantages = torch.zeros_like(rewards).to(device)\n",
                "        lastgaelam = 0\n",
                "        for t in reversed(range(num_steps)):\n",
                "            if t == num_steps - 1:\n",
                "                nextnonterminal = 1.0 - next_done\n",
                "                nextvalues = next_value\n",
                "            else:\n",
                "                nextnonterminal = 1.0 - dones[t + 1]\n",
                "                nextvalues = values[t + 1]\n",
                "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
                "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
                "        returns = advantages + values\n",
                "\n",
                "    # Flatten the batch\n",
                "    b_obs = obs.reshape((-1,) + env.single_observation_space.shape)\n",
                "    b_logprobs = logprobs.reshape(-1)\n",
                "    b_actions = actions.reshape((-1,) + env.single_action_space.shape)\n",
                "    b_advantages = advantages.reshape(-1)\n",
                "    b_returns = returns.reshape(-1)\n",
                "    b_values = values.reshape(-1)\n",
                "\n",
                "    # Optimizing the policy and value network\n",
                "    b_inds = np.arange(num_steps)\n",
                "    clipfracs = []\n",
                "    for epoch in range(num_epochs):\n",
                "        np.random.shuffle(b_inds)\n",
                "        for start in range(0, num_steps, minibatch_size):\n",
                "            end = start + minibatch_size\n",
                "            mb_inds = b_inds[start:end]\n",
                "\n",
                "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
                "            logratio = newlogprob - b_logprobs[mb_inds]\n",
                "            ratio = logratio.exp()\n",
                "\n",
                "            with torch.no_grad():\n",
                "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
                "                old_approx_kl = (-logratio).mean()\n",
                "                approx_kl = ((ratio - 1) - logratio).mean()\n",
                "                clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
                "\n",
                "            mb_advantages = b_advantages[mb_inds]\n",
                "            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
                "\n",
                "            # Policy loss\n",
                "            pg_loss1 = -mb_advantages * ratio\n",
                "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
                "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
                "\n",
                "            # Value loss\n",
                "            newvalue = newvalue.view(-1)\n",
                "            v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
                "            v_clipped = b_values[mb_inds] + torch.clamp(\n",
                "                newvalue - b_values[mb_inds],\n",
                "                -clip_coef,\n",
                "                clip_coef,\n",
                "            )\n",
                "            v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
                "            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
                "            v_loss = 0.5 * v_loss_max.mean()\n",
                "\n",
                "            entropy_loss = entropy.mean()\n",
                "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
                "            optimizer.step()\n",
                "\n",
                "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
                "    var_y = np.var(y_true)\n",
                "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
                "\n",
                "    print(f\"Update {update}/{num_updates}, Global Step: {global_step}, Mean Reward: {rewards.sum().item()}, Value Loss: {v_loss.item()}, Policy Loss: {pg_loss.item()}\")\n",
                "\n",
                "env.close()"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
